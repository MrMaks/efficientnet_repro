{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms.v2.functional as VF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.datasets import CIFAR100, ImageFolder\n",
    "from torchvision.models import EfficientNet_B0_Weights, efficientnet_b0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"using {device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2147483647\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"cifar\"\n",
    "dataset = \"tiny\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset_tiny():\n",
    "    data_dir = Path(\"data/tiny/tiny-imagenet-200\")\n",
    "    train_dir = data_dir / \"train\"\n",
    "    val_dir = data_dir / \"val\"\n",
    "\n",
    "    if not data_dir.exists():\n",
    "        print(\"download start...\")\n",
    "        !wget -P 'data/tiny' http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "        print(\"unzip start...\")\n",
    "        !unzip -qq 'data/tiny/tiny-imagenet-200.zip' -d \"data/tiny\"\n",
    "        print(\"unzip end...\")\n",
    "\n",
    "        with open(data_dir / \"val\" / \"val_annotations.txt\") as fin:\n",
    "            data = fin.readlines()\n",
    "            im2class = {}\n",
    "            for line in data:\n",
    "                words = line.split(\"\\t\")\n",
    "                fname, imclass = words[0], words[1]\n",
    "                im2class[fname] = imclass\n",
    "\n",
    "        print(\"rename started...\")\n",
    "        for img, imgclass in im2class.items():\n",
    "            new_dir: Path = val_dir / imgclass\n",
    "            if not new_dir.exists():\n",
    "                new_dir.mkdir(parents=True)\n",
    "            if not (new_dir / img).exists():\n",
    "                shutil.move(val_dir / \"images\" / img, new_dir / img)\n",
    "\n",
    "        (val_dir / \"images\").rmdir()\n",
    "\n",
    "    return train_dir, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_transform = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms() # shared, no augmentations used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"tiny\":\n",
    "    train_dir, val_dir = download_dataset_tiny()\n",
    "    train_dataset_full = ImageFolder(train_dir, transform=train_test_transform)\n",
    "    test_dataset = ImageFolder(val_dir, transform=train_test_transform)\n",
    "    num_classes = 200\n",
    "elif dataset == \"cifar\":\n",
    "    path = \"./data/cifar100\"\n",
    "    train_dataset_full = CIFAR100(\n",
    "        root=path, train=True, transform=train_test_transform, download=True\n",
    "    )\n",
    "    test_dataset = CIFAR100(\n",
    "        root=path, train=False, transform=train_test_transform, download=True\n",
    "    )\n",
    "    num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train_dataset_full)),\n",
    "    random_state=SEED,\n",
    "    train_size=0.9,\n",
    "    # train_size=0.01,\n",
    "    # train_size = 3e-3,\n",
    "    # test_size=0.01,\n",
    "    stratify=train_dataset_full.targets,\n",
    ")\n",
    "\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset = Subset(train_dataset_full, val_indices)\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [train_dataset_full.targets[i] for i in train_dataset.indices]\n",
    "plt.hist(train_targets, bins=num_classes)\n",
    "plt.show()\n",
    "val_targets = [train_dataset_full.targets[i] for i in val_dataset.indices]\n",
    "plt.hist(val_targets, bins=num_classes)\n",
    "plt.show()\n",
    "plt.hist(test_dataset.targets, bins=num_classes)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "num_workers = 2\n",
    "\n",
    "# state changes with all 3 loaders reading\n",
    "data_loader_generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=bs,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    generator=data_loader_generator,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=bs,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    generator=data_loader_generator,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=bs,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    generator=data_loader_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.indices[:10])\n",
    "print(val_dataset.indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_dataset.indices).union(val_dataset.indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = torch.nn.Linear(1280, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(num_classes)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"cifar\":\n",
    "    # https://github.com/pytorch/ignite/blob/master/examples/notebooks/EfficientNet_Cifar100_finetuning.ipynb\n",
    "    # not sure where they got it from but works well, e5(0 indexed), 0.835 val acc, 0.8354 test\n",
    "    lr = 1e-2\n",
    "    optimizer = SGD(\n",
    "        params=[\n",
    "            {\"params\": model.features[0].parameters(), \"lr\": lr * 0.1},\n",
    "            {\"params\": model.features[1:].parameters(), \"lr\": lr * 0.2},\n",
    "            {\"params\": model.classifier.parameters(), \"lr\": lr},\n",
    "        ],\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.001,\n",
    "        nesterov=True,\n",
    "    )\n",
    "\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.975)\n",
    "\n",
    "elif dataset == \"tiny\":\n",
    "    # https://arxiv.org/pdf/1912.08136v2 section 5.1.5\n",
    "    lr = 1e-3\n",
    "    optimizer = SGD(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001,\n",
    "        nesterov=True,  # not clear from paper, reference nesterov in other places as inspiration...\n",
    "    )\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "else:\n",
    "    raise ValueError(\"no such dataset config!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_pred: torch.Tensor, y_true: torch.Tensor, bs: int) -> float:\n",
    "    return ((y_pred == y_true).sum() / bs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(\n",
    "    model: torch.nn.Module, val_loader: DataLoader\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): _description_\n",
    "        val_loader (DataLoader): _description_\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: loss, accuracy\n",
    "    \"\"\"\n",
    "    print(\"--------- running validation ---------\")\n",
    "    pre_validation_time = time.time()\n",
    "    total_loss, total_correct = 0.0, 0.0\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batchi, (X, Y_true) in enumerate(val_loader, start=1):\n",
    "            data_loading_duration_ms = (time.time() - end) * 1e3\n",
    "\n",
    "            pre_forward_time = time.time()\n",
    "\n",
    "            X, Y_true = X.to(device), Y_true.to(device)\n",
    "            output = model(X)  # (bs, num_classes)\n",
    "            loss = F.cross_entropy(output, Y_true)\n",
    "\n",
    "            post_forward_time = time.time()\n",
    "\n",
    "            loss = loss.item()\n",
    "            Y_pred = output.argmax(dim=1)  # (bs, )\n",
    "\n",
    "            correct = (Y_pred == Y_true).sum().item()\n",
    "            batch_acc = correct / len(Y_true)\n",
    "            total_loss += loss\n",
    "            total_correct += correct\n",
    "\n",
    "            metric_calc_duration_ms = (time.time() - post_forward_time) * 1e3\n",
    "            forward_duration_ms = (post_forward_time - pre_forward_time) * 1e3\n",
    "            batch_time = (\n",
    "                data_loading_duration_ms + forward_duration_ms + metric_calc_duration_ms\n",
    "            )\n",
    "            if batchi < 5 or batchi % 20 == 0:\n",
    "                print(\n",
    "                    \"validation {:>4}/{} | val_loss={:>7.4f} | val_acc={:>7.3f} | total time (ms) {:>8.2f} | times (ms) :: data load {:>7.2f}, forward {:>7.2f}, metric calc {:>7.2f}\".format(\n",
    "                        batchi,\n",
    "                        len(val_loader),\n",
    "                        loss,\n",
    "                        batch_acc,\n",
    "                        batch_time,\n",
    "                        data_loading_duration_ms,\n",
    "                        forward_duration_ms,\n",
    "                        metric_calc_duration_ms,\n",
    "                    )\n",
    "                )\n",
    "            end = time.time()\n",
    "\n",
    "        total_acc = (\n",
    "            total_correct / len(val_loader.dataset)\n",
    "        )\n",
    "        total_loss /= len(val_loader)\n",
    "        print(f\"Total validation set acc: {total_acc:.2f}\")\n",
    "        print(f\"Val loss average per batch: {total_loss:.4f}\")\n",
    "\n",
    "        validation_time_sec = time.time() - pre_validation_time\n",
    "        print(f\"Total validation time: {validation_time_sec:2f} sec\")\n",
    "\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "early_stopping = False  # really stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"optimizer_defaults\": optimizer.defaults,\n",
    "    \"scheduler\": {\n",
    "        \"type\": str(type(scheduler)),\n",
    "        \"base_lr\": scheduler.base_lrs,\n",
    "        \"gamma\": scheduler.gamma,\n",
    "    },\n",
    "    \"bs\": bs,\n",
    "    \"data\": {\n",
    "        \"dataset\": dataset,\n",
    "        \"train\": len(train_dataset),\n",
    "        \"val\": len(val_dataset),\n",
    "        \"test\": len(test_dataset),\n",
    "    },\n",
    "}\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_folder = Path(f\"drive/MyDrive/colab_outputs/{dataset}\")\n",
    "\n",
    "log_folder = Path(f\"checkpoints/models/{dataset}\")\n",
    "Path(log_folder).mkdir(exist_ok=True)\n",
    "\n",
    "checkpoint_path = log_folder / Path(f\"{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\")\n",
    "checkpoint_path.mkdir(parents=True)\n",
    "\n",
    "print(f\"Saving all logs to: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "best_val_epoch, best_val_loss = -1, torch.inf\n",
    "train_acc_buff, train_loss_buff, val_acc_buff, val_loss_buff = [], [], [], []\n",
    "\n",
    "test_loss_buff, test_acc_buff = [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    print(f\"\\n====== epoch {epoch} =========\\n\")\n",
    "\n",
    "    epoch_train_loss, epoch_train_correct = 0.0, 0.0\n",
    "\n",
    "    batch_times = []\n",
    "    end = time.time()\n",
    "    for batchi, (X, Y_true) in enumerate(train_loader, start=1):\n",
    "        data_loading_duration_ms = (time.time() - end) * 1e3\n",
    "        pre_forward_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        X, Y_true = X.to(device), Y_true.to(device)\n",
    "\n",
    "        output = model(X)  # (bs, classes)\n",
    "        loss = F.cross_entropy(output, Y_true)\n",
    "\n",
    "        post_forward_time = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        post_backward_time = time.time()\n",
    "        optimizer.step()\n",
    "        post_step_time = time.time()\n",
    "\n",
    "        forward_duration_ms = (post_forward_time - pre_forward_time) * 1e3\n",
    "        backward_duration_ms = (post_backward_time - post_forward_time) * 1e3\n",
    "        step_duration_ms = (post_step_time - post_backward_time) * 1e3\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Y_pred = output.argmax(dim=1)  # (bs,)\n",
    "            correct = (Y_pred == Y_true).sum().item()\n",
    "            acc = correct / len(Y_pred)\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_correct += correct\n",
    "            metric_update_duration_ms = (time.time() - post_step_time) * 1e3\n",
    "\n",
    "            batch_time = (\n",
    "                data_loading_duration_ms\n",
    "                + forward_duration_ms\n",
    "                + backward_duration_ms\n",
    "                + step_duration_ms\n",
    "                + metric_update_duration_ms\n",
    "            )\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "            if batchi < 5 or batchi % 20 == 0:\n",
    "                print(\n",
    "                    \"batch {:>4}/{} | loss = {:>7.4f} | acc = {:>7.4f} | total time (ms) {:>10.2f} | times (ms) :: data load {:>7.2f}, forward {:>7.2f}, backward {:>7.2f}, step {:>7.2f}, metrics update {:>7.2f} \".format(\n",
    "                        batchi,\n",
    "                        len(train_loader),\n",
    "                        loss.item(),\n",
    "                        acc,\n",
    "                        batch_time,\n",
    "                        data_loading_duration_ms,\n",
    "                        forward_duration_ms,\n",
    "                        backward_duration_ms,\n",
    "                        step_duration_ms,\n",
    "                        metric_update_duration_ms,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    epoch_train_acc = epoch_train_correct / len(train_loader.dataset)\n",
    "\n",
    "    val_loss, val_acc = run_validation(model, val_loader)\n",
    "\n",
    "    if early_stopping and val_loss > best_val_loss:\n",
    "        print(\n",
    "            f\"=== EARLY STOPPING === new loss: {val_loss}, previous best: {best_val_loss} at epoch {best_val_epoch}\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    model_save_time_ms = 0.0\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"New best result found!\")\n",
    "        best_val_loss = val_loss\n",
    "        best_val_epoch = epoch\n",
    "\n",
    "        pre_model_save_time = time.time()\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            checkpoint_path\n",
    "            / f\"model_e{epoch}_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_vacc_{str(round(val_acc, 2)).replace('.', 'p')}_vl_{str(round(val_loss, 2)).replace('.', 'p')}.pt\",\n",
    "        )\n",
    "        model_save_time_ms = (time.time() - pre_model_save_time) * 1e3\n",
    "\n",
    "    # update accumulators\n",
    "    train_acc_buff.append(epoch_train_acc)\n",
    "    train_loss_buff.append(epoch_train_loss)\n",
    "    val_acc_buff.append(val_acc)\n",
    "    val_loss_buff.append(val_loss)\n",
    "\n",
    "    epoch_duration_sec = time.time() - epoch_start\n",
    "    print(\n",
    "        \"Epoch summary: \\ntrain :: loss {:>7.4f} |  acc {:>7.4f} \\nval :: loss {:>7.4f} |  acc {:>7.4f} | best val loss so far {:>7.4f} at epoch {:>2}\".format(\n",
    "            epoch_train_loss,\n",
    "            epoch_train_acc,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "            best_val_loss,\n",
    "            best_val_epoch,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch time (sec) {epoch_duration_sec:.2f} ({epoch_duration_sec / 60:.2f} min), model save time (ms): {model_save_time_ms:.2f} , average batch time (ms): {sum(batch_times) / len(batch_times) :<2f}\"\n",
    "    )\n",
    "\n",
    "    print(\"++++++ test set eval +++++\")\n",
    "    test_loss, test_acc = run_validation(model, test_loader)\n",
    "    test_loss_buff.append(test_loss)\n",
    "    test_acc_buff.append(test_acc)\n",
    "    print(\"Test result: loss={:.3f}, acc={:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(train_loss_buff, label=\"train loss\")\n",
    "plt.plot(val_loss_buff, label=\"val loss\")\n",
    "plt.plot(test_loss_buff, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xticks(list(range(len(train_loss_buff))))\n",
    "plt.show()\n",
    "fig.savefig(checkpoint_path / \"losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_acc_buff, label=\"train acc\")\n",
    "plt.plot(val_acc_buff, label=\"val acc\")\n",
    "plt.plot(test_acc_buff, label=\"test acc\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xticks(list(range(len(train_acc_buff))))\n",
    "plt.show()\n",
    "fig.savefig(checkpoint_path / \"acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint_path / \"graphdata.json\", \"w\") as fout:\n",
    "    json.dump(\n",
    "        {\n",
    "            **{\n",
    "                \"losses\": {\n",
    "                    \"train\": train_loss_buff,\n",
    "                    \"val\": val_loss_buff,\n",
    "                    \"test\": test_loss_buff,\n",
    "                },\n",
    "                \"acc\": {\n",
    "                    \"train\": train_acc_buff,\n",
    "                    \"val\": val_acc_buff,\n",
    "                    \"test\": test_acc_buff,\n",
    "                },\n",
    "            },\n",
    "            **config,\n",
    "        },\n",
    "        fout,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magister",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
